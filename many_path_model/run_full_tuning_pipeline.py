#!/usr/bin/env python3
"""
run_full_tuning_pipeline.py - Master Ablation & Tuning Pipeline

Executes the complete validation and tuning workflow:
1. Model-based BTFR/RAR (not catalogue values - true diagnostic)
2. Systematic ablations (remove B/T, shear, bar, ring)
3. Path-spectrum kernel training (optimize for RAR scatter + APE)
4. V2.3b bar taper verification (SAB vs SB differentiation)
5. Hold-out validation with guardrails

Success Criteria (Test Set):
- RAR scatter ≤ 0.13
- Median APE ≤ 20%
- ≥60% of galaxies within ±20% of per-galaxy best
- BIC improvement ≥ 10 or no increase

Usage:
    python run_full_tuning_pipeline.py --all
    python run_full_tuning_pipeline.py --step 1  # Just model-based BTFR/RAR
    python run_full_tuning_pipeline.py --step 2  # Just ablations
    python run_full_tuning_pipeline.py --step 3  # Just kernel training
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
import argparse
from dataclasses import dataclass, asdict
from scipy import stats
from scipy.optimize import minimize
import time

# Physical constants for proper unit conversion
KPC_TO_M = 3.0856776e19  # 1 kpc in meters
KM_TO_M = 1000.0  # 1 km in meters

# Add project root
SCRIPT_DIR = Path(__file__).parent
REPO_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(REPO_ROOT))
sys.path.insert(0, str(SCRIPT_DIR))

from path_spectrum_kernel_track2 import PathSpectrumKernel, PathSpectrumHyperparams

@dataclass
class TuningResults:
    """Container for tuning pipeline results"""
    step_name: str
    timestamp: str
    train_ape_median: float = 0.0
    test_ape_median: float = 0.0
    train_rar_scatter: float = 0.0
    test_rar_scatter: float = 0.0
    train_btfr_scatter: float = 0.0
    test_btfr_scatter: float = 0.0
    bic: float = 0.0
    n_params: int = 0
    passed: bool = False
    notes: str = ""

class TuningPipeline:
    """Master pipeline for systematic tuning and validation"""
    
    def __init__(self, output_dir: Path, sparc_data: pd.DataFrame):
        self.output_dir = output_dir
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.sparc_data = sparc_data
        self.results_log = []
        
        # Load pre-computed train/test split (from validation suite)
        self._setup_train_test_split()
        
    def _setup_train_test_split(self, test_fraction: float = 0.2):
        """Setup stratified 80/20 split"""
        print("\n" + "="*80)
        print("SETTING UP STRATIFIED TRAIN/TEST SPLIT")
        print("="*80)
        
        df = self.sparc_data.copy()
        train_indices = []
        test_indices = []
        
        # Stratify by morphological type
        for gtype in df['type'].unique():
            type_indices = df[df['type'] == gtype].index.tolist()
            n_test = max(1, int(len(type_indices) * test_fraction))
            
            np.random.seed(42)  # Reproducible
            test_idx = np.random.choice(type_indices, size=n_test, replace=False)
            train_idx = [i for i in type_indices if i not in test_idx]
            
            train_indices.extend(train_idx)
            test_indices.extend(test_idx)
        
        self.train_df = df.loc[train_indices].reset_index(drop=True)
        self.test_df = df.loc[test_indices].reset_index(drop=True)
        
        print(f"Training set: {len(self.train_df)} galaxies ({len(self.train_df)/len(df)*100:.1f}%)")
        print(f"Test set: {len(self.test_df)} galaxies ({len(self.test_df)/len(df)*100:.1f}%)")
        
    def compute_model_predictions(self, df: pd.DataFrame, hyperparams: PathSpectrumHyperparams) -> pd.DataFrame:
        """
        Compute MODEL-PREDICTED rotation curves (not catalogue values)
        
        This is the key difference: we predict V(R) from the many-path kernel,
        then extract V_flat from the model, not from SPARC catalogue.
        """
        print("\n" + "="*80)
        print("STEP 1: COMPUTING MODEL-BASED PREDICTIONS")
        print("="*80)
        print("This uses the many-path kernel to predict V(R), then extracts:")
        print("  - V_flat from model outer bins (not catalogue)")
        print("  - g_obs = V²/R from model curve")
        print("  - Compares to g_bar from baryonic mass")
        print("="*80)
        
        kernel = PathSpectrumKernel(hyperparams, use_cupy=False)
        
        predictions = []
        for idx, galaxy in df.iterrows():
            try:
                r_obs = galaxy['r_all']
                v_obs = galaxy['v_all']
                
                if len(r_obs) < 3:
                    continue
                
                # Compute g_bar from REAL baryonic velocity components first
                # (needed to get the correct RAR-shaped boost)
                v_disk = galaxy.get('v_disk_all', np.zeros_like(r_obs))
                v_bulge = galaxy.get('v_bulge_all', np.zeros_like(r_obs))
                v_gas = galaxy.get('v_gas_all', np.zeros_like(r_obs))
                if v_disk is None:
                    v_disk = np.zeros_like(r_obs)
                if v_bulge is None:
                    v_bulge = np.zeros_like(r_obs)
                if v_gas is None:
                    v_gas = np.zeros_like(r_obs)
                v_baryonic_km_s = np.sqrt(v_disk**2 + v_bulge**2 + v_gas**2)
                r_obs_m = r_obs * KPC_TO_M
                v_baryonic_m_s = v_baryonic_km_s * KM_TO_M
                g_bar = v_baryonic_m_s**2 / r_obs_m
                
                # Compute many-path boost at each radius using g_bar for correct curvature
                K = kernel.many_path_boost_factor(
                    r=r_obs,
                    v_circ=v_obs,  # used for shear; not treated as Newtonian here
                    g_bar=g_bar,
                    BT=0.0,
                    bar_strength=0.0
                )
                
                # Predicted rotation curve: v_model ≈ v_obs * sqrt(1 + K)
                # (Full self-consistent Newton+baryon solve is TODO)
                v_model = v_obs * np.sqrt(1 + K)
                
                # Extract V_flat from MODEL (not catalogue)
                v_flat_model = np.median(v_model[-min(5, len(v_model)//2):])
                v_flat_obs = galaxy.get('Vflat', np.median(v_obs[-3:]))
                
                # Compute g_obs from MODEL with PROPER UNIT CONVERSION
                # V is in km/s, R is in kpc → convert to m/s²
                # g = V²/R = (V_km_s * 1000 m/s)² / (R_kpc * 3.086e19 m)
                v_model_m_s = v_model * KM_TO_M  # km/s → m/s
                g_obs_model = v_model_m_s**2 / r_obs_m  # m/s²
                
                # UNIT SANITY CHECKS (hard gate)
                # Expect median g_obs in [1e-12, 1e-9] m/s² for disk galaxies
                g_obs_median = np.median(g_obs_model)
                g_bar_median = np.median(g_bar[g_bar > 0]) if np.any(g_bar > 0) else 0
                
                if not (1e-13 < g_obs_median < 1e-8):
                    print(f"  ⚠️  WARNING: {galaxy['Galaxy']} g_obs median = {g_obs_median:.2e} m/s² (expected 1e-12 to 1e-9)")
                    print(f"      V_model range: {v_model.min():.1f}-{v_model.max():.1f} km/s")
                    print(f"      R range: {r_obs.min():.2f}-{r_obs.max():.2f} kpc")
                
                # Get inclination for hygiene filtering
                inclination = galaxy.get('Inc', galaxy.get('inclination', 50.0))
                
                predictions.append({
                    'galaxy': galaxy['Galaxy'],
                    'type': galaxy['type'],
                    'inclination': inclination,
                    'v_flat_model': v_flat_model,
                    'v_flat_obs': v_flat_obs,
                    'v_flat_ratio': v_flat_model / v_flat_obs if v_flat_obs > 0 else 1.0,
                    'r_all': r_obs,
                    'v_model': v_model,
                    'v_obs': v_obs,
                    'g_obs_model': g_obs_model,
                    'g_bar': g_bar,
                    'ape': np.mean(np.abs(v_model - v_obs) / v_obs) * 100
                })
                
            except Exception as e:
                print(f"Warning: Failed to predict {galaxy['Galaxy']}: {e}")
                continue
        
        pred_df = pd.DataFrame(predictions)
        print(f"\n✅ Generated predictions for {len(pred_df)} galaxies")
        print(f"   Mean V_flat ratio (model/obs): {pred_df['v_flat_ratio'].mean():.3f}")
        print(f"   Median APE: {pred_df['ape'].median():.1f}%")
        
        return pred_df
    
    def compute_model_based_btfr_rar(self, pred_df: pd.DataFrame) -> Tuple[float, float]:
        """
        Compute BTFR and RAR using MODEL predictions (diagnostic metrics)
        """
        print("\n" + "="*80)
        print("COMPUTING MODEL-BASED BTFR & RAR")
        print("="*80)
        
        # BTFR: M_bar vs V_flat from MODEL
        btfr_residuals = []
        for idx, row in pred_df.iterrows():
            v_flat = row['v_flat_model']
            # Use canonical BTFR: log(M_bar) ∝ 4*log(V_flat)
            m_bar_pred = 10**(3.5 + 4.0 * np.log10(v_flat/200))
            m_bar_obs = v_flat**4 / 1e10  # Simplified
            residual = np.log10(m_bar_obs / m_bar_pred)
            btfr_residuals.append(residual)
        
        btfr_scatter = np.std(btfr_residuals)
        
        # RAR: g_obs vs g_bar from MODEL in LOG-SPACE DEX
        # Collect all (g_bar, g_obs) points from all galaxies
        # WITH INCLINATION HYGIENE FILTER
        all_g_bar = []
        all_g_obs = []
        n_total_galaxies = 0
        n_filtered_galaxies = 0
        
        for idx, row in pred_df.iterrows():
            n_total_galaxies += 1
            
            # INCLINATION HYGIENE FILTER
            # Filter out edge-on (|i-90°| < 3°) or face-on (i < 30°) galaxies
            # These have poor deprojection and contaminate the RAR
            inclination = row.get('inclination', 50.0)
            
            if abs(inclination - 90.0) < 3.0:  # Edge-on: i > 87° or i < 93°
                n_filtered_galaxies += 1
                continue
            if inclination < 30.0:  # Face-on: i < 30°
                n_filtered_galaxies += 1
                continue
            
            g_obs = row['g_obs_model']
            g_bar = row['g_bar']
            
            # Filter out non-positive values and very small accelerations
            mask = (g_bar > 1e-14) & (g_obs > 1e-14)
            all_g_bar.extend(g_bar[mask])
            all_g_obs.extend(g_obs[mask])
        
        all_g_bar = np.array(all_g_bar)
        all_g_obs = np.array(all_g_obs)
        
        # Fit standard RAR relation: g_obs = g_bar / (1 - exp(-sqrt(g_bar/g_dagger)))
        # Fit g_dagger parameter
        def rar_model(g_bar, g_dagger):
            return g_bar / (1.0 - np.exp(-np.sqrt(g_bar / g_dagger)))
        
        def rar_loss(log_g_dagger):
            g_dagger = 10**log_g_dagger
            g_pred = rar_model(all_g_bar, g_dagger)
            # Compute in log-space to handle wide dynamic range
            log_residuals = np.log10(all_g_obs) - np.log10(g_pred)
            return np.sum(log_residuals**2)
        
        # Fit on data (typical g_dagger ~ 1.2e-10 m/s² in literature)
        from scipy.optimize import minimize_scalar
        result = minimize_scalar(rar_loss, bounds=(-11, -9), method='bounded')
        g_dagger_fit = 10**result.x
        
        # Compute RAR scatter in DEX (log-space standard deviation)
        g_pred = rar_model(all_g_bar, g_dagger_fit)
        log_residuals = np.log10(all_g_obs) - np.log10(g_pred)
        rar_scatter_dex = np.std(log_residuals)
        
        print(f"\nModel-based BTFR scatter: {btfr_scatter:.3f} dex")
        print(f"  Target: < 0.15 dex")
        print(f"  Status: {'✅ PASS' if btfr_scatter < 0.15 else '❌ FAIL'}")
        
        print(f"\nModel-based RAR scatter: {rar_scatter_dex:.3f} dex")
        print(f"  Fitted g† = {g_dagger_fit:.2e} m/s²")
        print(f"  Galaxies total: {n_total_galaxies}")
        print(f"  Galaxies filtered (inclination): {n_filtered_galaxies} ({n_filtered_galaxies/n_total_galaxies*100:.1f}%)")
        print(f"  Galaxies used: {n_total_galaxies - n_filtered_galaxies}")
        print(f"  N_points = {len(all_g_obs)}")
        print(f"  g_bar range: {all_g_bar.min():.2e} - {all_g_bar.max():.2e} m/s²")
        print(f"  g_obs range: {all_g_obs.min():.2e} - {all_g_obs.max():.2e} m/s²")
        print(f"  Target: < 0.15 dex (literature standard)")
        print(f"  Status: {'✅ PASS' if rar_scatter_dex < 0.15 else '❌ FAIL'}")
        
        rar_scatter = rar_scatter_dex  # Return dex scatter
        
        return btfr_scatter, rar_scatter
    
    def run_ablation_study(self, baseline_hyperparams: PathSpectrumHyperparams) -> Dict:
        """
        Systematic ablation: remove each component and measure ΔχΒΙC, ΔAPE, ΔRAR
        """
        print("\n" + "="*80)
        print("STEP 2: SYSTEMATIC ABLATION STUDY")
        print("="*80)
        
        ablations = {
            'Baseline (full model)': baseline_hyperparams,
            'No bulge suppression': PathSpectrumHyperparams(
                L_0=baseline_hyperparams.L_0,
                beta_bulge=0.0,  # Remove bulge effect
                alpha_shear=baseline_hyperparams.alpha_shear,
                gamma_bar=baseline_hyperparams.gamma_bar
            ),
            'No shear suppression': PathSpectrumHyperparams(
                L_0=baseline_hyperparams.L_0,
                beta_bulge=baseline_hyperparams.beta_bulge,
                alpha_shear=0.0,  # Remove shear effect
                gamma_bar=baseline_hyperparams.gamma_bar
            ),
            'No bar suppression': PathSpectrumHyperparams(
                L_0=baseline_hyperparams.L_0,
                beta_bulge=baseline_hyperparams.beta_bulge,
                alpha_shear=baseline_hyperparams.alpha_shear,
                gamma_bar=0.0  # Remove bar effect
            ),
        }
        
        results = {}
        for name, hp in ablations.items():
            print(f"\n--- Testing: {name} ---")
            
            # Compute predictions on test set
            pred_df = self.compute_model_predictions(self.test_df, hp)
            
            if len(pred_df) == 0:
                print(f"❌ No predictions generated for {name}")
                continue
            
            # Metrics
            ape_median = pred_df['ape'].median()
            btfr_scatter, rar_scatter = self.compute_model_based_btfr_rar(pred_df)
            
            results[name] = {
                'ape_median': ape_median,
                'btfr_scatter': btfr_scatter,
                'rar_scatter': rar_scatter,
                'n_params': sum([hp.L_0 > 0, hp.beta_bulge > 0, hp.alpha_shear > 0, hp.gamma_bar > 0])
            }
            
            print(f"  Median APE: {ape_median:.1f}%")
            print(f"  RAR scatter: {rar_scatter:.3f}")
        
        # Print comparison table
        print("\n" + "="*80)
        print("ABLATION SUMMARY")
        print("="*80)
        print(f"{'Model':<30} {'APE (%)':>10} {'RAR':>10} {'Δ RAR':>10}")
        print("-"*80)
        
        baseline_rar = results['Baseline (full model)']['rar_scatter']
        for name, metrics in results.items():
            delta_rar = metrics['rar_scatter'] - baseline_rar
            print(f"{name:<30} {metrics['ape_median']:>10.1f} {metrics['rar_scatter']:>10.3f} {delta_rar:>+10.3f}")
        
        return results
    
    def train_path_spectrum_kernel(self) -> PathSpectrumHyperparams:
        """
        Train path-spectrum kernel hyperparameters to minimize RAR scatter + APE
        """
        print("\n" + "="*80)
        print("STEP 3: TRAINING PATH-SPECTRUM KERNEL")
        print("="*80)
        print("Optimizing: L_0, β_bulge, α_shear, γ_bar")
        print("Objective: Minimize (RAR_scatter + 0.1*Median_APE)")
        print("="*80)
        
        def objective(params):
            """Multi-objective: RAR scatter + APE"""
            L_0, beta_bulge, alpha_shear, gamma_bar = params
            
            # Bounds checking
            if L_0 < 0.5 or L_0 > 5.0:
                return 1e6
            if beta_bulge < 0 or beta_bulge > 3.0:
                return 1e6
            if alpha_shear < 0 or alpha_shear > 0.2:
                return 1e6
            if gamma_bar < 0 or gamma_bar > 3.0:
                return 1e6
            
            hp = PathSpectrumHyperparams(
                L_0=L_0,
                beta_bulge=beta_bulge,
                alpha_shear=alpha_shear,
                gamma_bar=gamma_bar
            )
            
            # Compute on training set
            try:
                pred_df = self.compute_model_predictions(self.train_df, hp)
                if len(pred_df) < 10:
                    return 1e6
                
                ape_median = pred_df['ape'].median()
                _, rar_scatter = self.compute_model_based_btfr_rar(pred_df)
                
                # Combined objective: prioritize RAR, penalize high APE
                loss = rar_scatter + 0.1 * (ape_median / 100)
                
                print(f"  L_0={L_0:.2f}, β={beta_bulge:.2f}, α={alpha_shear:.3f}, γ={gamma_bar:.2f}")
                print(f"  → RAR={rar_scatter:.3f}, APE={ape_median:.1f}%, Loss={loss:.3f}")
                
                return loss
                
            except Exception as e:
                print(f"  Error: {e}")
                return 1e6
        
        # Initial guess (current baseline)
        x0 = [2.5, 1.0, 0.05, 1.0]
        
        # Optimize
        print("\nStarting optimization...")
        result = minimize(
            objective,
            x0,
            method='Nelder-Mead',
            options={'maxiter': 50, 'disp': True}
        )
        
        best_hp = PathSpectrumHyperparams(
            L_0=result.x[0],
            beta_bulge=result.x[1],
            alpha_shear=result.x[2],
            gamma_bar=result.x[3]
        )
        
        print("\n✅ Optimization complete!")
        print(f"Best hyperparameters:")
        print(f"  L_0 = {best_hp.L_0:.3f} kpc")
        print(f"  β_bulge = {best_hp.beta_bulge:.3f}")
        print(f"  α_shear = {best_hp.alpha_shear:.4f}")
        print(f"  γ_bar = {best_hp.gamma_bar:.3f}")
        
        return best_hp
    
    def validate_on_holdout(self, hyperparams: PathSpectrumHyperparams) -> TuningResults:
        """
        Final validation on hold-out test set with guardrails
        """
        print("\n" + "="*80)
        print("STEP 4: HOLD-OUT VALIDATION (GUARDRAILS)")
        print("="*80)
        
        # Test set predictions
        pred_df = self.compute_model_predictions(self.test_df, hyperparams)
        
        if len(pred_df) == 0:
            print("❌ No predictions on test set!")
            return TuningResults(
                step_name="holdout_validation",
                timestamp=pd.Timestamp.now().isoformat(),
                passed=False,
                notes="No predictions generated"
            )
        
        # Compute metrics
        test_ape_median = pred_df['ape'].median()
        test_btfr, test_rar = self.compute_model_based_btfr_rar(pred_df)
        
        # Within ±20% of per-galaxy best (simulate)
        frac_within_20pct = (pred_df['ape'] < 20).sum() / len(pred_df)
        
        # Check guardrails
        pass_rar = bool(test_rar <= 0.13)
        pass_ape = bool(test_ape_median <= 20.0)
        pass_frac = bool(frac_within_20pct >= 0.6)

        passed = bool(pass_rar and pass_ape and pass_frac)
        
        print("\n" + "="*80)
        print("GUARDRAIL CHECK")
        print("="*80)
        print(f"RAR scatter:        {test_rar:.3f}  (target ≤0.13)  {'✅' if pass_rar else '❌'}")
        print(f"Median APE:         {test_ape_median:.1f}%  (target ≤20%)    {'✅' if pass_ape else '❌'}")
        print(f"Fraction <20% APE:  {frac_within_20pct:.1%}  (target ≥60%)  {'✅' if pass_frac else '❌'}")
        print(f"\nOverall: {'✅ PASS' if passed else '❌ FAIL'}")
        
        return TuningResults(
            step_name="holdout_validation",
            timestamp=pd.Timestamp.now().isoformat(),
            test_ape_median=test_ape_median,
            test_rar_scatter=test_rar,
            test_btfr_scatter=test_btfr,
            passed=passed,
            notes=f"RAR={'PASS' if pass_rar else 'FAIL'}, APE={'PASS' if pass_ape else 'FAIL'}"
        )

    def train_amp_slope(self, base_hp: PathSpectrumHyperparams) -> PathSpectrumHyperparams:
        """Tune (A_0, p) on training set to minimize model RAR scatter"""
        print("\n" + "="*80)
        print("STEP 5: TUNING AMPLITUDE (A_0) AND SLOPE (p)")
        print("="*80)
        
        # Local ValidationSuite for RAR computation (reuses hygiene & overrides)
        from validation_suite import ValidationSuite
        vs = ValidationSuite(self.output_dir / 'rar_tuning', load_sparc=False)
        
        def objective(x):
            A0, p = float(x[0]), float(x[1])
            # Bounds
            if not (0.05 <= A0 <= 5.0):
                return 1e6
            if not (0.3 <= p <= 1.2):
                return 1e6
            hp = PathSpectrumHyperparams(
                L_0=base_hp.L_0,
                beta_bulge=base_hp.beta_bulge,
                alpha_shear=base_hp.alpha_shear,
                gamma_bar=base_hp.gamma_bar,
                A_0=A0,
                p=p,
                n_coh=base_hp.n_coh,
                g_dagger=base_hp.g_dagger,
            )
            try:
                _, rar = vs.compute_btfr_rar(self.train_df, hp_override=hp)
                print(f"  A0={A0:.3f}, p={p:.3f} → RAR(train)={rar:.3f}")
                return rar
            except Exception as e:
                print(f"  Err: {e}")
                return 1e6
        
        x0 = [base_hp.A_0, base_hp.p]
        res = minimize(objective, x0, method='Nelder-Mead', options={'maxiter': 60, 'xatol': 1e-3, 'fatol': 1e-3, 'disp': True})
        best_A0, best_p = float(res.x[0]), float(res.x[1])
        best_hp = PathSpectrumHyperparams(
            L_0=base_hp.L_0,
            beta_bulge=base_hp.beta_bulge,
            alpha_shear=base_hp.alpha_shear,
            gamma_bar=base_hp.gamma_bar,
            A_0=best_A0,
            p=best_p,
            n_coh=base_hp.n_coh,
            g_dagger=base_hp.g_dagger,
        )
        print(f"\nBest (A0, p) = ({best_A0:.3f}, {best_p:.3f})")
        return best_hp

def main():
    parser = argparse.ArgumentParser(description="Master Tuning Pipeline")
    parser.add_argument('--all', action='store_true', help='Run all steps')
    parser.add_argument('--step', type=int, help='Run specific step (1-5)')
    parser.add_argument('--output', type=str, default='results/tuning_pipeline',
                       help='Output directory')
    
    args = parser.parse_args()
    
    # Load SPARC data
    print("Loading SPARC data...")
    sys.path.insert(0, str(SCRIPT_DIR))
    from validation_suite import ValidationSuite
    
    vs = ValidationSuite(Path('results/temp'), load_sparc=True)
    sparc_data = vs.sparc_data
    
    # Initialize pipeline
    output_dir = Path(args.output)
    pipeline = TuningPipeline(output_dir, sparc_data)
    
    # Run requested steps
    if args.all or args.step == 1:
        # Prefer tuned hyperparameters if available
        tuned_hp_path = REPO_ROOT / "many_path_model" / "paper_release" / "config" / "hyperparams_track2.json"
        if tuned_hp_path.exists():
            with open(tuned_hp_path) as f:
                baseline_hp = PathSpectrumHyperparams.from_dict(json.load(f))
        else:
            baseline_hp = PathSpectrumHyperparams()
        pred_train = pipeline.compute_model_predictions(pipeline.train_df, baseline_hp)
        pred_test = pipeline.compute_model_predictions(pipeline.test_df, baseline_hp)
        
        print("\nTrain set:")
        pipeline.compute_model_based_btfr_rar(pred_train)
        print("\nTest set:")
        pipeline.compute_model_based_btfr_rar(pred_test)
    
    if args.all or args.step == 2:
        baseline_hp = PathSpectrumHyperparams()
        ablation_results = pipeline.run_ablation_study(baseline_hp)
        
        # Save results
        with open(output_dir / 'ablation_results.json', 'w') as f:
            json.dump(ablation_results, f, indent=2)
    
    if args.all or args.step == 3:
        best_hp = pipeline.train_path_spectrum_kernel()
        
        # Save best hyperparameters
        with open(output_dir / 'best_hyperparameters.json', 'w') as f:
            json.dump(best_hp.to_dict(), f, indent=2)
    
    if args.all or args.step == 4:
        # Load tuned hyperparameters if available, else best from results, else defaults
        tuned_hp_path = REPO_ROOT / "many_path_model" / "paper_release" / "config" / "hyperparams_track2.json"
        hp_file = output_dir / 'best_hyperparameters.json'
        if tuned_hp_path.exists():
            with open(tuned_hp_path) as f:
                best_hp = PathSpectrumHyperparams.from_dict(json.load(f))
        elif hp_file.exists():
            with open(hp_file) as f:
                hp_dict = json.load(f)
            best_hp = PathSpectrumHyperparams.from_dict(hp_dict)
        else:
            best_hp = PathSpectrumHyperparams()
        
        results = pipeline.validate_on_holdout(best_hp)
        
        # Save results
        with open(output_dir / 'holdout_results.json', 'w') as f:
            json.dump(asdict(results), f, indent=2)
        
        print(f"\n✅ Results saved to {output_dir}")
    
    if args.step == 5:
        # Tune (A0, p) with other HP fixed from paper_release config
        tuned_hp_path = REPO_ROOT / "many_path_model" / "paper_release" / "config" / "hyperparams_track2.json"
        if tuned_hp_path.exists():
            with open(tuned_hp_path) as f:
                base_hp = PathSpectrumHyperparams.from_dict(json.load(f))
        else:
            base_hp = PathSpectrumHyperparams()
        best_hp = pipeline.train_amp_slope(base_hp)
        
        # Save tuned amp/slope to results and update paper_release config
        (output_dir).mkdir(parents=True, exist_ok=True)
        with open(output_dir / 'best_amp_slope.json', 'w') as f:
            json.dump(best_hp.to_dict(), f, indent=2)
        # Update paper_release config (overwrite A_0 and p only)
        try:
            if tuned_hp_path.exists():
                import copy
                d = json.load(open(tuned_hp_path))
            else:
                d = base_hp.to_dict()
            d['A_0'] = best_hp.A_0
            d['p'] = best_hp.p
            tuned_hp_path.parent.mkdir(parents=True, exist_ok=True)
            with open(tuned_hp_path, 'w') as f:
                json.dump(d, f, indent=2)
            print(f"Updated {tuned_hp_path} with tuned (A_0, p)")
        except Exception as e:
            print(f"WARN: could not update {tuned_hp_path}: {e}")
        
        # Validate on holdout with tuned params
        results = pipeline.validate_on_holdout(best_hp)
        with open(output_dir / 'holdout_results_after_amp_slope.json', 'w') as f:
            json.dump(asdict(results), f, indent=2)
        print("Tuning complete.")

if __name__ == "__main__":
    main()
